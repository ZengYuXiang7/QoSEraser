{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import dgl as d\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets.dataset import FeatureLookup\n",
    "from models.node2vec import Node2vecModel as Node2vec2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "__coding__ = \"utf-8\"\n",
    "__author__ = \"Ng WaiMing\"\n",
    "\n",
    "from numpy import *\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "def distEclud(vecA, vecB):\n",
    "    \"\"\"\n",
    "    欧氏距离计算函数\n",
    "    :param vecA:\n",
    "    :param vecB:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return sqrt(sum(power(vecA - vecB, 2)))\n",
    "\n",
    "\n",
    "def randCent(dataMat, k):\n",
    "    \"\"\"\n",
    "    为给定数据集构建一个包含K个随机质心的集合,\n",
    "    随机质心必须要在整个数据集的边界之内,这可以通过找到数据集每一维的最小和最大值来完成\n",
    "    然后生成0到1.0之间的随机数并通过取值范围和最小值,以便确保随机点在数据的边界之内\n",
    "    :param dataMat:\n",
    "    :param k:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 获取样本数与特征值\n",
    "    m, n = shape(dataMat)\n",
    "    # 初始化质心,创建(k,n)个以零填充的矩阵\n",
    "    centroids = mat(zeros((k, n)))\n",
    "    # 循环遍历特征值\n",
    "    for j in range(n):\n",
    "        # 计算每一列的最小值\n",
    "        minJ = min(dataMat[:, j])\n",
    "        # 计算每一列的范围值\n",
    "        rangeJ = float(max(dataMat[:, j]) - minJ)\n",
    "        # 计算每一列的质心,并将值赋给centroids\n",
    "        centroids[:, j] = mat(minJ + rangeJ * random.rand(k, 1))\n",
    "    # 返回质心\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def kMeans(dataMat, k, distMeas = distEclud, createCent = randCent):\n",
    "    '''\n",
    "    创建K个质心,然后将每个店分配到最近的质心,再重新计算质心。\n",
    "    这个过程重复数次,直到数据点的簇分配结果不再改变为止\n",
    "    :param dataMat: 数据集\n",
    "    :param k: 簇的数目\n",
    "    :param distMeans: 计算距离\n",
    "    :param createCent: 创建初始质心\n",
    "    :return:\n",
    "    '''\n",
    "    # 获取样本数和特征数\n",
    "    m, n = shape(dataMat)\n",
    "    # 初始化一个矩阵来存储每个点的簇分配结果\n",
    "    # clusterAssment包含两个列:一列记录簇索引值,第二列存储误差(误差是指当前点到簇质心的距离,后面会使用该误差来评价聚类的效果)\n",
    "    clusterAssment = mat(zeros((m, 2)))\n",
    "    # 创建质心,随机K个质心\n",
    "    centroids = createCent(dataMat, k)\n",
    "    # 初始化标志变量,用于判断迭代是否继续,如果True,则继续迭代\n",
    "\n",
    "    clusterChanged = True\n",
    "    while clusterChanged:\n",
    "        clusterChanged = False\n",
    "        # 遍历所有数据找到距离每个点最近的质心,\n",
    "        # 可以通过对每个点遍历所有质心并计算点到每个质心的距离来完成\n",
    "        for i in range(m):\n",
    "            minDist = inf\n",
    "            minIndex = -1\n",
    "            for j in range(k):\n",
    "                # 计算数据点到质心的距离\n",
    "                # 计算距离是使用distMeas参数给出的距离公式,默认距离函数是distEclud\n",
    "                distJI = distMeas(centroids[j, :], dataMat[i, :])\n",
    "                # 如果距离比minDist(最小距离)还小,更新minDist(最小距离)和最小质心的index(索引)\n",
    "                if distJI < minDist:\n",
    "                    minDist = distJI\n",
    "                    minIndex = j\n",
    "            # 如果任一点的簇分配结果发生改变,则更新clusterChanged标志\n",
    "            if clusterAssment[i, 0] != minIndex:\n",
    "                clusterChanged = True\n",
    "            # 更新簇分配结果为最小质心的index(索引),minDist(最小距离)的平方\n",
    "            clusterAssment[i, :] = minIndex, minDist ** 2\n",
    "\n",
    "        # print(centroids)\n",
    "\n",
    "        # 遍历所有质心并更新它们的取值\n",
    "        for cent in range(k):\n",
    "            # 通过数据过滤来获得给定簇的所有点\n",
    "            ptsInClust = dataMat[nonzero(clusterAssment[:, 0].A == cent)[0]]\n",
    "            # 计算所有点的均值,axis=0表示沿矩阵的列方向进行均值计算\n",
    "            centroids[cent, :] = mean(ptsInClust, axis=0)\n",
    "\n",
    "    # 返回所有的类质心与点分配结果\n",
    "    return centroids, clusterAssment\n",
    "\n",
    "\n",
    "def biKmeans(dataMat, k, distMeas = distEclud):\n",
    "    '''\n",
    "    在给定数据集,所期望的簇数目和距离计算方法的条件下,函数返回聚类结果\n",
    "    :param dataMat:\n",
    "    :param k:\n",
    "    :param distMeas:\n",
    "    :return:\n",
    "    '''\n",
    "    m, n = shape(dataMat)\n",
    "    # 创建一个矩阵来存储数据集中每个点的簇分配结果及平方误差\n",
    "    clusterAssment = mat(zeros((m, 2)))\n",
    "\n",
    "    # 计算整个数据集的质心,并使用一个列表来保留所有的质心\n",
    "    centroid0 = mean(dataMat, axis = 0).tolist()[0]\n",
    "    centList = [centroid0]\n",
    "\n",
    "    # 遍历数据集中所有点来计算每个点到质心的误差值\n",
    "    for j in range(m):\n",
    "        clusterAssment[j, 1] = distMeas(mat(centroid0), dataMat[j, :]) ** 2\n",
    "\n",
    "    # 对簇不停的进行划分,直到得到想要的簇数目为止\n",
    "    while len(centList) < k:\n",
    "        # 初始化最小SSE为无穷大,用于比较划分前后的SSE\n",
    "        lowestSSE = inf\n",
    "        # 通过考察簇列表中的值来获得当前簇的数目,遍历所有的簇来决定最佳的簇进行划分\n",
    "        for i in range(len(centList)):\n",
    "            # 对每一个簇,将该簇中的所有点堪称一个小的数据集\n",
    "            ptsInCurrCluster = dataMat[nonzero(clusterAssment[:, 0].A == i)[0], :]\n",
    "\n",
    "            # 将ptsInCurrCluster输入到函数kMeans中进行处理,k=2,\n",
    "            # kMeans会生成两个质心(簇),同时给出每个簇的误差值\n",
    "            centroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas)\n",
    "\n",
    "            # 将误差值与剩余数据集的误差之和作为本次划分的误差\n",
    "            sseSplit = sum(splitClustAss[:, 1])\n",
    "            sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:, 0].A != i)[0], 1])\n",
    "            # print('sseSplit, and notSplit: ', sseSplit, sseNotSplit)\n",
    "\n",
    "            # 如果本次划分的SSE值最小,则本次划分被保存\n",
    "            if (sseSplit + sseNotSplit) < lowestSSE:\n",
    "                bestCentToSplit = i\n",
    "                bestNewCents = centroidMat\n",
    "                bestClustAss = splitClustAss.copy()\n",
    "                lowestSSE = sseSplit + sseNotSplit\n",
    "\n",
    "        # 找出最好的簇分配结果\n",
    "        # 调用kmeans函数并且指定簇数为2时,会得到两个编号分别为0和1的结果簇\n",
    "        bestClustAss[nonzero(bestClustAss[:, 0].A == 1)[0], 0] = len(centList)\n",
    "        # 更新为最佳质心\n",
    "        bestClustAss[nonzero(bestClustAss[:, 0].A == 0)[0], 0] = bestCentToSplit\n",
    "\n",
    "        print(len(centList))\n",
    "        print('the bestCentToSplit is: ', bestCentToSplit)\n",
    "        print('the len of bestClustAss is: ', len(bestClustAss))\n",
    "\n",
    "        # 更新质心列表\n",
    "        # 更新原质心list中的第i个质心为使用二分kMeans后bestNewCents的第一个质心\n",
    "        centList[bestCentToSplit] = bestNewCents[0, :].tolist()[0]\n",
    "\n",
    "        # 添加bestNewCents的第二个质心\n",
    "        centList.append(bestNewCents[1, :].tolist()[0])\n",
    "\n",
    "        # 重新分配最好簇下的数据(质心)以及SSEk\n",
    "        clusterAssment[nonzero(clusterAssment[:, 0].A == bestCentToSplit)[0], :] = bestClustAss\n",
    "\n",
    "    return mat(centList), clusterAssment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class a:\n",
    "    def __init__(self, node2vec_dim, node2vec_length, node2vec_walk, node2vec_epochs, node2vec_batchsize):\n",
    "        self.node2vec_dim = node2vec_dim\n",
    "        self.node2vec_length = node2vec_length\n",
    "        self.node2vec_walk = node2vec_walk\n",
    "        self.node2vec_epochs = node2vec_epochs\n",
    "        self.node2vec_batchsize = node2vec_batchsize\n",
    "        self.slices = 10\n",
    "\n",
    "class FeatureLookup:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.inner_id_counter = 0\n",
    "        self.inner_bag = {}\n",
    "        self.category = set()\n",
    "        self.category_bags = {}\n",
    "        self.inverse_map = {}\n",
    "\n",
    "    def register(self, category, value):\n",
    "        # 添加进入类别\n",
    "        self.category.add(category)\n",
    "\n",
    "        # 如果类别不存在若无则，则新增一个类别子树\n",
    "        if category not in self.category_bags:\n",
    "            self.category_bags[category] = {}\n",
    "\n",
    "        # 如果值不在全局索引中，则创建之，id += 1\n",
    "        if value not in self.inner_bag:\n",
    "            self.inner_bag[value] = self.inner_id_counter\n",
    "            self.inverse_map[self.inner_id_counter] = value\n",
    "            # 如果值不存在与类别子树，则创建之\n",
    "            if value not in self.category_bags[category]:\n",
    "                self.category_bags[category][value] = self.inner_id_counter\n",
    "            self.inner_id_counter += 1\n",
    "\n",
    "    def query_id(self, value):\n",
    "        # 返回索引id\n",
    "        return self.inner_bag[value]\n",
    "\n",
    "    def query_value(self, idx):\n",
    "        # 返回值\n",
    "        return self.inverse_map[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inner_bag)\n",
    "\n",
    "def node2vec_embedings(graph, args):\n",
    "\n",
    "    # config\n",
    "    p = 1\n",
    "    q = 0.5\n",
    "    embedding_dim = args.node2vec_dim\n",
    "    walk_length = args.node2vec_length\n",
    "    num_walks = args.node2vec_walk\n",
    "    epochs = args.node2vec_epochs\n",
    "    batch_size = args.node2vec_batchsize\n",
    "    device = 'gpu'\n",
    "\n",
    "    # config\n",
    "    trainer = Node2vec2(\n",
    "        graph,\n",
    "        embedding_dim = embedding_dim,\n",
    "        walk_length = walk_length,\n",
    "        p = p,\n",
    "        q = q,\n",
    "        num_walks = num_walks,\n",
    "        # weight_name = 'w',\n",
    "        device = device,\n",
    "    )\n",
    "\n",
    "    trainer.train(epochs = epochs, batch_size = batch_size, learning_rate = 1e-2)\n",
    "\n",
    "    node_features = trainer.embedding()\n",
    "\n",
    "    nodes = node_features.detach().to('cpu').numpy()\n",
    "\n",
    "    return nodes\n",
    "\n",
    "# node2vec\n",
    "def eraser(datasets, args):\n",
    "    label = None\n",
    "    userg = d.graph([])\n",
    "    user_lookup = FeatureLookup()\n",
    "    ufile = pd.read_csv('./datasets/data/WSDREAM/原始数据/userlist_table.csv')\n",
    "    ufile = pd.DataFrame(ufile)\n",
    "    ulines = ufile.to_numpy()\n",
    "\n",
    "    for i in range(339):\n",
    "        user_lookup.register('User', i)\n",
    "\n",
    "    for ure in ulines[:, 2]:\n",
    "        user_lookup.register('URE', ure)\n",
    "\n",
    "    for uas in ulines[:, 4]:\n",
    "        user_lookup.register('UAS', uas)\n",
    "\n",
    "    userg.add_nodes(len(user_lookup))\n",
    "\n",
    "    for line in ulines:\n",
    "        uid = line[0]\n",
    "        ure = user_lookup.query_id(line[2])\n",
    "\n",
    "        if not userg.has_edges_between(uid, ure):\n",
    "            userg.add_edges(uid, ure)\n",
    "\n",
    "        uas = user_lookup.query_id(line[4])\n",
    "        if not userg.has_edges_between(uid, uas):\n",
    "            userg.add_edges(uid, uas)\n",
    "\n",
    "    userg = d.add_self_loop(userg)\n",
    "    userg = d.to_bidirected(userg)\n",
    "\n",
    "    # node2vec\n",
    "    ans = None\n",
    "    ans = np.array(node2vec_embedings(userg, args))\n",
    "    ans = ans[: 339]\n",
    "\n",
    "    # 整理特征\n",
    "    df = np.array(ans)\n",
    "\n",
    "    pk.dump(df, open(f'./pretrain/user_embeds.pk', 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "args = a(128, 20, 50, 2000, 32)\n",
    "# eraser(None, args)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "the bestCentToSplit is:  0\n",
      "the len of bestClustAss is:  339\n",
      "2\n",
      "the bestCentToSplit is:  1\n",
      "the len of bestClustAss is:  330\n",
      "3\n",
      "the bestCentToSplit is:  1\n",
      "the len of bestClustAss is:  311\n",
      "4\n",
      "the bestCentToSplit is:  2\n",
      "the len of bestClustAss is:  19\n",
      "5\n",
      "the bestCentToSplit is:  3\n",
      "the len of bestClustAss is:  9\n",
      "6\n",
      "the bestCentToSplit is:  2\n",
      "the len of bestClustAss is:  15\n",
      "7\n",
      "the bestCentToSplit is:  1\n",
      "the len of bestClustAss is:  302\n",
      "8\n",
      "the bestCentToSplit is:  7\n",
      "the len of bestClustAss is:  7\n",
      "9\n",
      "the bestCentToSplit is:  6\n",
      "the len of bestClustAss is:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yxzeng/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/yxzeng/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    }
   ],
   "source": [
    "df = pk.load(open(f'./pretrain/user_embeds.pk', 'rb'))\n",
    "myCentroids, clustAssing = biKmeans(df, 10, distMeas = distEclud)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "data = np.array(clustAssing[:,0], dtype = 'int32').reshape(-1,)\n",
    "data = data.tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "{1: 295, 4: 4, 0: 9, 7: 4, 8: 3, 2: 3, 6: 6, 5: 6, 3: 3, 9: 6}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = {}\n",
    "for i in data:\n",
    "    if i not in dic:\n",
    "        dic[i] = 1\n",
    "    else:\n",
    "        dic[i] += 1\n",
    "dic"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "339"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = 0\n",
    "for i in dic:\n",
    "    ans += dic[i]\n",
    "ans"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class a:\n",
    "    def __init__(self, node2vec_dim, node2vec_length, node2vec_walk, node2vec_epochs, node2vec_batchsize):\n",
    "        self.node2vec_dim = node2vec_dim\n",
    "        self.node2vec_length = node2vec_length\n",
    "        self.node2vec_walk = node2vec_walk\n",
    "        self.node2vec_epochs = node2vec_epochs\n",
    "        self.node2vec_batchsize = node2vec_batchsize\n",
    "        self.slices = 10\n",
    "        self.part_iter = 100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# 欧氏距离计算\n",
    "\n",
    "\n",
    "# user-based-parition\n",
    "def user_based_parition(tensor, args):\n",
    "    \"\"\"\n",
    "        part_iter :: 切分设定\n",
    "        slices :: 切分片数\n",
    "    \"\"\"\n",
    "    def E_score2(value1, value2):\n",
    "        return np.sum(np.power(value1 - value2, 2))\n",
    "\n",
    "    user_embeds = pk.load(open('./pretrain/user_embeds.pk', 'rb'))\n",
    "\n",
    "    split_Tensor = []\n",
    "\n",
    "    max_number = 1.2 * user_embeds.shape[0] // args.slices\n",
    "\n",
    "    import random\n",
    "    center_id = random.sample(range(0, user_embeds.shape[0]), args.slices)\n",
    "\n",
    "    center_user_value = []\n",
    "    for i in range(args.slices):\n",
    "        center_user_value.append(user_embeds[center_id[i]])\n",
    "\n",
    "    C = None\n",
    "    for iterid in range(args.part_iter):\n",
    "        C = [{} for _ in range(args.slices)]\n",
    "        Scores = {}\n",
    "        for userid in range(user_embeds.shape[0]):\n",
    "            for sliceid in range(args.slices):\n",
    "                score = E_score2(np.array(user_embeds[userid]), np.array(center_user_value[sliceid]))\n",
    "                Scores[userid, sliceid] = -score\n",
    "        Scores = sorted(Scores.items(), key = lambda x : x[1], reverse = True)\n",
    "\n",
    "        visted = [False for _ in range(user_embeds.shape[0])]\n",
    "        for i in range(len(Scores)):\n",
    "            if not visted[Scores[i][0][0]]:\n",
    "                if len(C[Scores[i][0][1]]) < max_number:\n",
    "                    C[Scores[i][0][1]][Scores[i][0][0]] = tensor[Scores[i][0][0]]\n",
    "                    visted[Scores[i][0][0]] = True\n",
    "\n",
    "        center_user_value_next = []\n",
    "        for sliceid in range(args.slices):\n",
    "            temp_user_value = []\n",
    "\n",
    "            for userid in C[sliceid].keys():\n",
    "                temp_user_value.append(user_embeds[userid])\n",
    "\n",
    "            if len(temp_user_value):\n",
    "                center_user_value_next.append(np.mean(temp_user_value))\n",
    "            else:\n",
    "                center_user_value_next.append(0)\n",
    "\n",
    "        loss = 0.\n",
    "\n",
    "        for sliceid in range(args.slices):\n",
    "            score = E_score2(np.array(center_user_value_next[sliceid]), np.array(center_user_value[sliceid]))\n",
    "            loss += score\n",
    "\n",
    "        center_user_value = center_user_value_next\n",
    "        # log(f'iterid {iterid} loss : {loss:.50f}')\n",
    "\n",
    "    # 开始返回339 * 5825矩阵\n",
    "    for sliceid in range(args.slices):\n",
    "        splits_Tensor = np.zeros_like(tensor)\n",
    "\n",
    "        idx = list(C[sliceid].keys())\n",
    "        splits_Tensor[idx] = tensor[idx]\n",
    "\n",
    "        split_Tensor.append(splits_Tensor)  # 1\n",
    "\n",
    "    return split_Tensor, C"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "tensor = np.array(pk.load(open('datasets/data/WSDREAM/rt.pk', 'rb')))\n",
    "args = a(128, 20, 50, 1000, 32)\n",
    "split_Tensor, C = user_based_parition(tensor, args)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "40\n",
      "40\n",
      "33\n",
      "10\n",
      "40\n",
      "40\n",
      "16\n",
      "40\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "for i in C:\n",
    "    print(len(i))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "1974675"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = 0\n",
    "for i in range(10):\n",
    "    ans += (np.array(split_Tensor[i]) != 0).sum()\n",
    "ans"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# 欧氏距离计算\n",
    "\n",
    "\n",
    "# user-based-parition\n",
    "def item_based_parition(tensor, args):\n",
    "    \"\"\"\n",
    "        part_iter :: 切分设定\n",
    "        slices :: 切分片数\n",
    "    \"\"\"\n",
    "    def E_score2(value1, value2):\n",
    "        return np.sum(np.power(value1 - value2, 2))\n",
    "\n",
    "    item_embeds = pk.load(open('./pretrain/item_embeds.pk', 'rb'))\n",
    "\n",
    "    split_Tensor = []\n",
    "\n",
    "    max_number = 1.2 * item_embeds.shape[0] // args.slices\n",
    "\n",
    "    import random\n",
    "    center_id = random.sample(range(0, item_embeds.shape[0]), args.slices)\n",
    "\n",
    "    center_user_value = []\n",
    "    for i in range(args.slices):\n",
    "        center_user_value.append(item_embeds[center_id[i]])\n",
    "\n",
    "    C = None\n",
    "    for iterid in range(args.part_iter):\n",
    "        C = [{} for _ in range(args.slices)]\n",
    "        Scores = {}\n",
    "        for userid in range(item_embeds.shape[0]):\n",
    "            for sliceid in range(args.slices):\n",
    "                score = E_score2(np.array(item_embeds[userid]), np.array(center_user_value[sliceid]))\n",
    "                Scores[userid, sliceid] = -score\n",
    "        Scores = sorted(Scores.items(), key = lambda x : x[1], reverse = True)\n",
    "\n",
    "        visted = [False for _ in range(item_embeds.shape[0])]\n",
    "        for i in range(len(Scores)):\n",
    "            if not visted[Scores[i][0][0]]:\n",
    "                if len(C[Scores[i][0][1]]) < max_number:\n",
    "                    C[Scores[i][0][1]][Scores[i][0][0]] = tensor[:,Scores[i][0][0]]\n",
    "                    visted[Scores[i][0][0]] = True\n",
    "\n",
    "        center_user_value_next = []\n",
    "        for sliceid in range(args.slices):\n",
    "            temp_user_value = []\n",
    "\n",
    "            for userid in C[sliceid].keys():\n",
    "                temp_user_value.append(item_embeds[userid])\n",
    "\n",
    "            if len(temp_user_value):\n",
    "                center_user_value_next.append(np.mean(temp_user_value))\n",
    "            else:\n",
    "                center_user_value_next.append(0)\n",
    "\n",
    "        loss = 0.\n",
    "\n",
    "        for sliceid in range(args.slices):\n",
    "            score = E_score2(np.array(center_user_value_next[sliceid]), np.array(center_user_value[sliceid]))\n",
    "            loss += score\n",
    "\n",
    "        center_user_value = center_user_value_next\n",
    "        # log(f'iterid {iterid} loss : {loss:.50f}')\n",
    "\n",
    "    # 开始返回339 * 5825矩阵\n",
    "    for sliceid in range(args.slices):\n",
    "        splits_Tensor = np.zeros_like(tensor)\n",
    "\n",
    "        idx = list(C[sliceid].keys())\n",
    "        splits_Tensor[:,idx] = tensor[:,idx]\n",
    "\n",
    "        split_Tensor.append(splits_Tensor)  # 1\n",
    "\n",
    "    return split_Tensor, C"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "tensor = np.array(pk.load(open('datasets/data/WSDREAM/rt.pk', 'rb')))\n",
    "args = a(128, 20, 50, 1000, 32)\n",
    "# split_Tensor, C = item_based_parition(tensor, args)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "40\n",
      "40\n",
      "33\n",
      "10\n",
      "40\n",
      "40\n",
      "16\n",
      "40\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "for i in C:\n",
    "    print(len(i))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "1974675"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = 0\n",
    "for i in range(10):\n",
    "    ans += (np.array(split_Tensor[i]) != 0).sum()\n",
    "ans"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from tqdm import *\n",
    "# RecEarser\n",
    "def interaction_based_balanced_parition(tensor, args):\n",
    "    def E_score2(value1, value2):\n",
    "        return np.sum(np.power(value1 - value2, 2))\n",
    "\n",
    "    user_embeds = pk.load(open('./pretrain/user_embeds.pk', 'rb'))\n",
    "    item_embeds = pk.load(open('./pretrain/item_embeds.pk', 'rb'))\n",
    "\n",
    "    split_Tensor = []\n",
    "\n",
    "    data = []\n",
    "    for i in range(tensor.shape[0]):\n",
    "        for j in range(tensor.shape[1]):\n",
    "            data.append([i, j])\n",
    "\n",
    "    max_number = 1.2 * (tensor != 0).sum() // args.slices\n",
    "    # max_number = 1.2 * Tensor.shape[0] // args.slices\n",
    "    import random\n",
    "    center_id = random.sample(data, args.slices)\n",
    "\n",
    "    # center_id = random.sample(range(0, tensor.shape[0]), args.slices)\n",
    "\n",
    "    center_user_value = []\n",
    "    for i in range(args.slices):\n",
    "        center_user_value.append([user_embeds[center_id[i][0]], item_embeds[center_id[i][1]]])\n",
    "\n",
    "    C = None\n",
    "    for iterid in trange(args.part_iter):\n",
    "        C = [{} for _ in range(args.slices)]\n",
    "        C_number = [0 for _ in range(args.slices)]\n",
    "        Scores = {}\n",
    "\n",
    "        for userid in range(len(data)):\n",
    "            for sliceid in range(args.slices):\n",
    "                score_user = E_score2(np.array(user_embeds[data[userid][0]]), np.array(center_user_value[sliceid][0]))\n",
    "                score_item = E_score2(np.array(item_embeds[data[userid][0]]), np.array(center_user_value[sliceid][1]))\n",
    "                Scores[userid, sliceid] = - score_user * score_item\n",
    "\n",
    "        Scores = sorted(Scores.items(), key = lambda x : x[1], reverse = True)\n",
    "\n",
    "        visted = [False for _ in range(len(data))]\n",
    "        for i in range(len(Scores)):\n",
    "            if not visted[Scores[i][0][0]]:\n",
    "                if C_number[Scores[i][0][1]] < max_number:\n",
    "                    if data[Scores[i][0][0]][0] not in C[Scores[i][0][1]]:\n",
    "                        C[Scores[i][0][1]][data[Scores[i][0][0]][0]] = [data[Scores[i][0][0]][1]]  # 把这个item放入对应的user元祖\n",
    "                    else:\n",
    "                        C[Scores[i][0][1]][data[Scores[i][0][0]][0]].append(data[Scores[i][0][0]][1])\n",
    "\n",
    "                    # print(C[Scores[i][0][1]][data[Scores[i][0][0]][0]])\n",
    "\n",
    "                    visted[Scores[i][0][0]] = True\n",
    "                    C_number[Scores[i][0][1]] += 1\n",
    "\n",
    "        center_user_value_next = []\n",
    "\n",
    "        for sliceid in range(args.slices):\n",
    "            temp_user_value = []\n",
    "            temp_item_value = []\n",
    "            user_mean, item_mean = None, None\n",
    "            for userid in C[sliceid].keys():\n",
    "                for itemid in C[sliceid][userid]:\n",
    "                    temp_user_value.append(user_embeds[userid])\n",
    "                    temp_item_value.append(item_embeds[itemid])\n",
    "                    if len(temp_user_value):\n",
    "                        user_mean = np.mean(temp_user_value)\n",
    "                    else:\n",
    "                        user_mean = 0\n",
    "\n",
    "                    if len(temp_item_value):\n",
    "                        item_mean = np.mean(temp_item_value)\n",
    "                    else:\n",
    "                        item_mean = 0\n",
    "            center_user_value_next.append([user_mean, item_mean])\n",
    "\n",
    "        loss = 0.0\n",
    "\n",
    "        for sliceid in range(args.slices):\n",
    "            score_user = E_score2(np.array(center_user_value_next[sliceid][0]), np.array(center_user_value[sliceid][0]))\n",
    "            score_item = E_score2(np.array(center_user_value_next[sliceid][1]), np.array(center_user_value[sliceid][1]))\n",
    "            loss += (score_user * score_item)\n",
    "\n",
    "        center_user_value = center_user_value_next\n",
    "        log(f'iterid {iterid + 1} : loss = {loss:.30f}')\n",
    "        for sliceid in range(args.slices):\n",
    "            log(f'C[{sliceid}] number = {len(list(C[sliceid]))}')\n",
    "\n",
    "    row_idx = [[] for _ in range(args.slices)]\n",
    "    col_idx = [[] for _ in range(args.slices)]\n",
    "\n",
    "    for sliceid in range(args.slices):\n",
    "        temp = np.zeros_like(tensor)\n",
    "\n",
    "        for userid in C[sliceid].keys():\n",
    "            row_idx[sliceid] += [userid for _ in range(len(C[sliceid][userid]))]\n",
    "            col_idx[sliceid] += [itemid for itemid in C[sliceid][userid]]\n",
    "\n",
    "        temp[row_idx[sliceid], col_idx[sliceid]] = tensor[row_idx[sliceid], col_idx[sliceid]]\n",
    "\n",
    "        split_Tensor.append(temp)\n",
    "\n",
    "    return split_Tensor, C"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [3:00:52<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-20-1e5745022b57>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0margs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0ma\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m128\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m20\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m50\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1000\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m32\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpart_iter\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m20\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0msplit_Tensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mC\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minteraction_based_balanced_parition\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-18-ef7c61e76a8a>\u001B[0m in \u001B[0;36minteraction_based_balanced_parition\u001B[0;34m(tensor, args)\u001B[0m\n\u001B[1;32m     65\u001B[0m                     \u001B[0mtemp_item_value\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem_embeds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mitemid\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtemp_user_value\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 67\u001B[0;31m                         \u001B[0muser_mean\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtemp_user_value\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     68\u001B[0m                     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m                         \u001B[0muser_mean\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/core/overrides.py\u001B[0m in \u001B[0;36mmean\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001B[0m in \u001B[0;36mmean\u001B[0;34m(a, axis, dtype, out, keepdims, where)\u001B[0m\n\u001B[1;32m   3430\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3431\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3432\u001B[0;31m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001B[0m\u001B[1;32m   3433\u001B[0m                           out=out, **kwargs)\n\u001B[1;32m   3434\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py\u001B[0m in \u001B[0;36m_mean\u001B[0;34m(a, axis, dtype, out, keepdims, where)\u001B[0m\n\u001B[1;32m    162\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    163\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_mean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkeepdims\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwhere\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 164\u001B[0;31m     \u001B[0marr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0masanyarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    165\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    166\u001B[0m     \u001B[0mis_float16_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "tensor = np.array(pk.load(open('datasets/data/WSDREAM/rt.pk', 'rb')))\n",
    "args = a(128, 20, 50, 1000, 32)\n",
    "args.part_iter = 20\n",
    "split_Tensor, C = interaction_based_balanced_parition(tensor, args)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "C = np.array(C)\n",
    "pk.dump(C, file = open('C.pk', 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
