{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import dgl as d\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets.dataset import FeatureLookup\n",
    "from models.node2vec import Node2vecModel as Node2vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class a:\n",
    "    def __init__(self, node2vec_dim, node2vec_length, node2vec_walk, node2vec_epochs, node2vec_batchsize):\n",
    "        self.node2vec_dim = node2vec_dim\n",
    "        self.node2vec_length = node2vec_length\n",
    "        self.node2vec_walk = node2vec_walk\n",
    "        self.node2vec_epochs = node2vec_epochs\n",
    "        self.node2vec_batchsize = node2vec_batchsize\n",
    "        self.slices = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [03:11<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-26 19:09:52  \tNode2vec loss = 0.7851\n"
     ]
    }
   ],
   "source": [
    "def node2vec_embedings(graph, args):\n",
    "\n",
    "    # config\n",
    "    p = 1\n",
    "    q = 0.5\n",
    "    embedding_dim = args.node2vec_dim\n",
    "    walk_length = args.node2vec_length\n",
    "    num_walks = args.node2vec_walk\n",
    "    epochs = args.node2vec_epochs\n",
    "    batch_size = args.node2vec_batchsize\n",
    "    device = 'gpu'\n",
    "\n",
    "    # config\n",
    "    trainer = Node2vec2(\n",
    "        graph,\n",
    "        embedding_dim = embedding_dim,\n",
    "        walk_length = walk_length,\n",
    "        p = p,\n",
    "        q = q,\n",
    "        num_walks = num_walks,\n",
    "        # weight_name = 'w',\n",
    "        device = device,\n",
    "    )\n",
    "\n",
    "    trainer.train(epochs = epochs, batch_size = batch_size, learning_rate = 1e-2)\n",
    "\n",
    "    node_features = trainer.embedding()\n",
    "\n",
    "    nodes = node_features.detach().to('cpu').numpy()\n",
    "\n",
    "    return nodes\n",
    "\n",
    "# node2vec 基于服务切分\n",
    "def eraser2(datasets, args):\n",
    "    label = None\n",
    "\n",
    "    servg = d.graph([])\n",
    "\n",
    "    serv_lookup = FeatureLookup()\n",
    "\n",
    "    sfile = pd.read_csv('./datasets/data/WSDREAM/原始数据/wslist_table.csv')\n",
    "    sfile = pd.DataFrame(sfile)\n",
    "    slines = sfile.to_numpy()\n",
    "\n",
    "    for i in range(5825):\n",
    "        serv_lookup.register('Sid', i)\n",
    "\n",
    "    for sre in slines[:, 4]:\n",
    "        serv_lookup.register('SRE', sre)\n",
    "\n",
    "    for spr in slines[:, 2]:\n",
    "        serv_lookup.register('SPR', spr)\n",
    "\n",
    "    for sas in slines[:, 6]:\n",
    "        serv_lookup.register('SAS', sas)\n",
    "\n",
    "    servg.add_nodes(len(serv_lookup))\n",
    "\n",
    "    for line in slines:\n",
    "        sid = line[0]\n",
    "        sre = serv_lookup.query_id(line[4])\n",
    "        if not servg.has_edges_between(sid, sre):\n",
    "            servg.add_edges(sid, sre)\n",
    "\n",
    "        sas = serv_lookup.query_id(line[6])\n",
    "        if not servg.has_edges_between(sid, sas):\n",
    "            servg.add_edges(sid, sas)\n",
    "\n",
    "        spr = serv_lookup.query_id(line[2])\n",
    "        if not servg.has_edges_between(sid, spr):\n",
    "            servg.add_edges(sid, spr)\n",
    "\n",
    "    servg = d.add_self_loop(servg)\n",
    "    servg = d.to_bidirected(servg)\n",
    "\n",
    "    # node2vec\n",
    "    ans = None\n",
    "    ans = np.array(node2vec_embedings(servg, args))\n",
    "    ans = ans[: 5825]\n",
    "\n",
    "    df = np.array(ans)\n",
    "    pk.dump(df, open(f'./pretrain/item_embeds.pk', 'wb'))\n",
    "\n",
    "    def k_mean(inputs, k):\n",
    "        from sklearn.cluster import KMeans\n",
    "        kmeans = KMeans(n_clusters = k)\n",
    "        kmeans.fit(inputs)\n",
    "        return kmeans.labels_\n",
    "\n",
    "    label = k_mean(df, 10)\n",
    "    pk.dump(label, open(f'./pretrain/item_based_{args.slices}.pk', 'wb'))\n",
    "#\n",
    "args = a(128, 10, 50, 150, 512)\n",
    "eraser2(None, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node2vec\n",
    "def eraser(datasets, args):\n",
    "    label = None\n",
    "    userg = d.graph([])\n",
    "    user_lookup = FeatureLookup()\n",
    "    ufile = pd.read_csv('./datasets/data/WSDREAM/原始数据/userlist_table.csv')\n",
    "    ufile = pd.DataFrame(ufile)\n",
    "    ulines = ufile.to_numpy()\n",
    "\n",
    "    for i in range(339):\n",
    "        user_lookup.register('User', i)\n",
    "\n",
    "    for ure in ulines[:, 2]:\n",
    "        user_lookup.register('URE', ure)\n",
    "\n",
    "    for uas in ulines[:, 4]:\n",
    "        user_lookup.register('UAS', uas)\n",
    "\n",
    "    userg.add_nodes(len(user_lookup))\n",
    "\n",
    "    for line in ulines:\n",
    "        uid = line[0]\n",
    "        ure = user_lookup.query_id(line[2])\n",
    "\n",
    "        if not userg.has_edges_between(uid, ure):\n",
    "            userg.add_edges(uid, ure)\n",
    "\n",
    "        uas = user_lookup.query_id(line[4])\n",
    "        if not userg.has_edges_between(uid, uas):\n",
    "            userg.add_edges(uid, uas)\n",
    "\n",
    "    userg = d.add_self_loop(userg)\n",
    "    userg = d.to_bidirected(userg)\n",
    "\n",
    "    # node2vec\n",
    "    ans = None\n",
    "    ans = np.array(node2vec_embedings(userg, args))\n",
    "    ans = ans[: 339]\n",
    "\n",
    "    # 整理特征\n",
    "    df = np.array(ans)\n",
    "\n",
    "    def k_mean(inputs, k):\n",
    "        from sklearn.cluster import KMeans\n",
    "        kmeans = KMeans(n_clusters = k)\n",
    "        kmeans.fit(inputs)\n",
    "        return kmeans.labels_\n",
    "\n",
    "    label = k_mean(df, 10)\n",
    "    pk.dump(label, open(f'./pretrain/user_based_{args.slices}.pk', 'wb'))\n",
    "\n",
    "# def __init__(self, node2vec_dim, node2vec_length, node2vec_walk, node2vec_epochs, node2vec_batchsize):\n",
    "args = a(128, 20, 50, 1000, 32)\n",
    "# eraser(None, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb7c85c17b1976aa789c9d87b9763b6f005911cd226333300bbe1880120eaefe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
