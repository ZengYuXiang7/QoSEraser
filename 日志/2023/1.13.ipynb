{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset..\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1873745/1873745 [00:02<00:00, 637746.88it/s]\n",
      "/home/yxzeng/anaconda3/lib/python3.8/site-packages/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Finish..\n",
      "\n",
      "Registered\n",
      "Nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 717167/1873745 [19:26<31:20, 614.88it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Hong Kong'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-ca1ea42bd1dc>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    191\u001B[0m         \u001B[0mextg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_edges\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0muid\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msid\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    192\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 193\u001B[0;31m     \u001B[0mure\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mextlookup\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mquery_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mure\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    194\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mextg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhas_edges_between\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0muid\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mure\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m         \u001B[0mextg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_edges\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0muid\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mure\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/code_python/RecEraser_GPU_NeuCF_GraphMF_CSMF/utils.py\u001B[0m in \u001B[0;36mquery_id\u001B[0;34m(self, value)\u001B[0m\n\u001B[1;32m     40\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mquery_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0;31m# 返回索引id\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 42\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__inner_bag\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     43\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mquery_value\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mid\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'Hong Kong'"
     ]
    }
   ],
   "source": [
    "# ====================================================#\n",
    "# Import Package & Setting Environment\n",
    "# ====================================================#\n",
    "\n",
    "import dgl as d\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from utils import QoSGraphDataset,FeatureLookup\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import *\n",
    "import dgl.function as fn\n",
    "log = logging.getLogger('log')\n",
    "t.set_default_tensor_type(t.FloatTensor)\n",
    "\n",
    "def Metrics(realVec, estiVec):\n",
    "    realVec = np.array(realVec)\n",
    "    estiVec = np.array(estiVec)\n",
    "\n",
    "    absError = np.abs(estiVec - realVec)\n",
    "    mae = np.mean(absError)\n",
    "    nmae = mae / (np.sum(realVec) / absError.shape[0])\n",
    "    rmse = np.linalg.norm(absError) / np.sqrt(absError.shape[0])\n",
    "    relativeError = absError / realVec\n",
    "    mre = np.percentile(relativeError, 50)\n",
    "    npre = np.percentile(relativeError, 90)\n",
    "\n",
    "    return np.array([mae, nmae, rmse, mre, npre])\n",
    "\n",
    "# ====================================================#\n",
    "# Experiment Setting\n",
    "# ====================================================#\n",
    "density = 0.025\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# ====================================================#\n",
    "# Import Dataset & Create Feature Lookup\n",
    "# ====================================================#\n",
    "\n",
    "dataset = QoSGraphDataset('rtds.txt',type='rt')\n",
    "#\n",
    "trainsize = int(density * 339*5825)\n",
    "testsize = len(dataset) - trainsize\n",
    "trainset, testset = random_split(dataset, [trainsize, testsize])\n",
    "testset, _ = random_split(testset, [100000, len(testset) - 100000])\n",
    "# ====================================================#\n",
    "# Create Graph\n",
    "# ====================================================#\n",
    "def get_predefine_graph(dataset):\n",
    "\n",
    "    userg = d.DGLGraph()\n",
    "    servg = d.DGLGraph()\n",
    "    user_lookup = FeatureLookup()\n",
    "    serv_lookup = FeatureLookup()\n",
    "    for i in range(339):\n",
    "        user_lookup.register('User',i)\n",
    "    for j in range(5825):\n",
    "        serv_lookup.register('Serv', j)\n",
    "    for ure in dataset.data[:,3]:\n",
    "        user_lookup.register('URE', ure)\n",
    "    for uas in dataset.data[:, 4]:\n",
    "        user_lookup.register('UAS', uas)\n",
    "    for sre in dataset.data[:, 5]:\n",
    "        serv_lookup.register('SRE', sre)\n",
    "    for spr in dataset.data[:, 6]:\n",
    "        serv_lookup.register('SPR', spr)\n",
    "    for sas in dataset.data[:, 7]:\n",
    "        serv_lookup.register('SAS', sas)\n",
    "\n",
    "    print('Registered')\n",
    "    userg.add_nodes(len(user_lookup))\n",
    "    servg.add_nodes(len(serv_lookup))\n",
    "    print('Nodes')\n",
    "\n",
    "    for line in tqdm(dataset.data):\n",
    "        uid, sid, val, ure, uas, sre, spr, sas = line\n",
    "\n",
    "        ure = user_lookup.query_id(ure)\n",
    "        if not userg.has_edges_between(uid,ure):\n",
    "            userg.add_edges(uid, ure)\n",
    "\n",
    "        uas = user_lookup.query_id(uas)\n",
    "        if not userg.has_edges_between(uid,uas):\n",
    "            userg.add_edges(uid, uas)\n",
    "\n",
    "        sre = serv_lookup.query_id(sre)\n",
    "        if not servg.has_edges_between(sid,sre):\n",
    "            servg.add_edges(sid, sre)\n",
    "\n",
    "        sas = serv_lookup.query_id(sas)\n",
    "        if not servg.has_edges_between(sid,sas):\n",
    "            servg.add_edges(sid, sas)\n",
    "\n",
    "        spr = serv_lookup.query_id(spr)\n",
    "        if not servg.has_edges_between(sid,spr):\n",
    "            servg.add_edges(sid, spr)\n",
    "\n",
    "    userg = d.add_self_loop(userg)\n",
    "    userg = d.to_bidirected(userg)\n",
    "    servg = d.add_self_loop(servg)\n",
    "    servg = d.to_bidirected(servg)\n",
    "\n",
    "\n",
    "def get_one_graph(dataset):\n",
    "    extg = d.DGLGraph()\n",
    "    extlookup = FeatureLookup()\n",
    "\n",
    "    for i in range(339):\n",
    "        extlookup.register('User',i)\n",
    "    for j in range(5825):\n",
    "        extlookup.register('Serv', j)\n",
    "    for ure in dataset.data[:,2]:\n",
    "        extlookup.register('RE', ure)\n",
    "    for uas in dataset.data[:, 4]:\n",
    "        extlookup.register('UAS', uas)\n",
    "\n",
    "    for sre in dataset.data[:, 5]:\n",
    "        extlookup.register('RE', sre)\n",
    "    for spr in dataset.data[:, 6]:\n",
    "        extlookup.register('SPR', spr)\n",
    "    for sas in dataset.data[:, 7]:\n",
    "        extlookup.register('SAS', sas)\n",
    "\n",
    "    print('Registered')\n",
    "    extg.add_nodes(len(extlookup))\n",
    "    print('Nodes')\n",
    "\n",
    "    for line in tqdm(dataset.data):\n",
    "        uid, sid, val, ure, uas, sre, spr, sas = line\n",
    "        uid = int(uid)\n",
    "        sid = int(sid) + 339\n",
    "\n",
    "        if not extg.has_edges_between(uid,sid):\n",
    "            extg.add_edges(uid, sid)\n",
    "\n",
    "        ure = extlookup.query_id(ure)\n",
    "        if not extg.has_edges_between(uid,ure):\n",
    "            extg.add_edges(uid, ure)\n",
    "\n",
    "        uas = extlookup.query_id(uas)\n",
    "        if not extg.has_edges_between(uid,uas):\n",
    "            extg.add_edges(uid, uas)\n",
    "\n",
    "        sre = extlookup.query_id(sre)\n",
    "        if not extg.has_edges_between(sid,sre):\n",
    "            extg.add_edges(sid, sre)\n",
    "\n",
    "        sas = extlookup.query_id(sas)\n",
    "        if not extg.has_edges_between(sid,sas):\n",
    "            extg.add_edges(sid, sas)\n",
    "\n",
    "        spr = extlookup.query_id(spr)\n",
    "        if not extg.has_edges_between(sid,spr):\n",
    "            extg.add_edges(sid, spr)\n",
    "\n",
    "    extg = d.add_self_loop(extg)\n",
    "    extg = d.to_bidirected(extg)\n",
    "\n",
    "extg = d.DGLGraph()\n",
    "extlookup = FeatureLookup()\n",
    "\n",
    "for i in range(339):\n",
    "    extlookup.register('User',i)\n",
    "for j in range(5825):\n",
    "    extlookup.register('Serv', j)\n",
    "for ure in dataset.data[:,2]:\n",
    "    extlookup.register('RE', ure)\n",
    "for uas in dataset.data[:, 4]:\n",
    "    extlookup.register('UAS', uas)\n",
    "\n",
    "for sre in dataset.data[:, 5]:\n",
    "    extlookup.register('RE', sre)\n",
    "for spr in dataset.data[:, 6]:\n",
    "    extlookup.register('SPR', spr)\n",
    "for sas in dataset.data[:, 7]:\n",
    "    extlookup.register('SAS', sas)\n",
    "\n",
    "print('Registered')\n",
    "extg.add_nodes(len(extlookup))\n",
    "print('Nodes')\n",
    "\n",
    "for line in tqdm(dataset.data):\n",
    "    uid, sid, val, ure, uas, sre, spr, sas = line\n",
    "    uid = int(uid)\n",
    "    sid = int(sid) + 339\n",
    "\n",
    "    if not extg.has_edges_between(uid,sid):\n",
    "        extg.add_edges(uid, sid)\n",
    "\n",
    "    ure = extlookup.query_id(ure)\n",
    "    if not extg.has_edges_between(uid,ure):\n",
    "        extg.add_edges(uid, ure)\n",
    "\n",
    "    uas = extlookup.query_id(uas)\n",
    "    if not extg.has_edges_between(uid,uas):\n",
    "        extg.add_edges(uid, uas)\n",
    "\n",
    "    sre = extlookup.query_id(sre)\n",
    "    if not extg.has_edges_between(sid,sre):\n",
    "        extg.add_edges(sid, sre)\n",
    "\n",
    "    sas = extlookup.query_id(sas)\n",
    "    if not extg.has_edges_between(sid,sas):\n",
    "        extg.add_edges(sid, sas)\n",
    "\n",
    "    spr = extlookup.query_id(spr)\n",
    "    if not extg.has_edges_between(sid,spr):\n",
    "        extg.add_edges(sid, spr)\n",
    "\n",
    "extg = d.add_self_loop(extg)\n",
    "extg = d.to_bidirected(extg)\n",
    "# ====================================================#\n",
    "# Create DataLoader\n",
    "# ====================================================#\n",
    "testset, _ = random_split(testset, [40000, len(testset) - 40000])\n",
    "trainLoader = DataLoader(trainset, 256, shuffle=True,num_workers=6,pin_memory=True)\n",
    "testLoader = DataLoader(testset, 256, shuffle=True,num_workers=6,pin_memory=True)\n",
    "dataset.switch()\n",
    "\n",
    "# ====================================================#\n",
    "# Create Model\n",
    "# ====================================================#\n",
    "from dgl.nn import SAGEConv,GATConv\n",
    "\n",
    "class NeuralCF(t.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralCF, self).__init__()\n",
    "        self.layers = t.nn.Sequential(\n",
    "            t.nn.Linear(input_size, 128),\n",
    "            t.nn.LayerNorm(128),\n",
    "            t.nn.ReLU(),\n",
    "            t.nn.Linear(128, 128),\n",
    "            t.nn.LayerNorm(128),\n",
    "            t.nn.ReLU(),\n",
    "            t.nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layers(input)\n",
    "\n",
    "\n",
    "class GraphSAGEConv(t.nn.Module):\n",
    "\n",
    "    def __init__(self, graph, dim, order=3):\n",
    "        super(GraphSAGEConv, self).__init__()\n",
    "        self.order = order\n",
    "        self.graph = graph\n",
    "        self.embedding = t.nn.Parameter(t.Tensor(self.graph.number_of_nodes(), dim))\n",
    "        t.nn.init.kaiming_normal_(self.embedding)\n",
    "        self.graph.ndata['L0'] = self.embedding\n",
    "        self.layers = t.nn.ModuleList([SAGEConv(dim, dim, aggregator_type='gcn') for _ in range(order)])\n",
    "        #self.layers = t.nn.ModuleList([GATConv(dim, dim, num_heads=1) for _ in range(order)])\n",
    "        self.norms = t.nn.ModuleList([t.nn.LayerNorm(dim) for _ in range(order)])\n",
    "        self.acts = t.nn.ModuleList([t.nn.ReLU() for _ in range(order)])\n",
    "\n",
    "    def forward(self,uid):\n",
    "        g = self.graph\n",
    "        feats = g.ndata['L0']\n",
    "        for i, (layer, norm, act) in enumerate(zip(self.layers, self.norms, self.acts)):\n",
    "            feats = layer(g,feats).squeeze()\n",
    "            feats = norm(feats)\n",
    "            feats = act(feats)\n",
    "            g.ndata[f'L{i + 1}'] = feats\n",
    "        embeds = g.ndata['L0'][uid]\n",
    "        for i in range(self.order):\n",
    "            embeds = t.cat((embeds, g.ndata[f'L{i + 1}'][uid]), dim=-1)\n",
    "\n",
    "        return embeds\n",
    "\n",
    "class GraphSAGEConvSG(t.nn.Module):\n",
    "\n",
    "    def __init__(self, graph, dim, order=3):\n",
    "        super(GraphSAGEConvSG, self).__init__()\n",
    "        self.order = order\n",
    "        self.graph = graph\n",
    "        self.embedding = t.nn.Parameter(t.Tensor(self.graph.number_of_nodes(), dim))\n",
    "        t.nn.init.kaiming_normal_(self.embedding)\n",
    "        self.graph.ndata['L0'] = self.embedding\n",
    "        self.layers = t.nn.ModuleList([SAGEConv(dim, dim, aggregator_type='gcn') for _ in range(order)])\n",
    "        #self.layers = t.nn.ModuleList([GATConv(dim, dim, num_heads=1) for _ in range(order)])\n",
    "        self.norms = t.nn.ModuleList([t.nn.LayerNorm(dim) for _ in range(order)])\n",
    "        self.acts = t.nn.ModuleList([t.nn.ReLU() for _ in range(order)])\n",
    "\n",
    "    def forward(self,uid,sid):\n",
    "        g = self.graph\n",
    "        feats = g.ndata['L0']\n",
    "        for i, (layer, norm, act) in enumerate(zip(self.layers, self.norms, self.acts)):\n",
    "            feats = layer(g,feats).squeeze()\n",
    "            feats = norm(feats)\n",
    "            feats = act(feats)\n",
    "            g.ndata[f'L{i + 1}'] = feats\n",
    "        user_embeds = g.ndata['L0'][uid]\n",
    "        serv_embeds = g.ndata['L0'][sid+339]\n",
    "\n",
    "        for i in range(self.order):\n",
    "            user_embeds = t.cat((user_embeds, g.ndata[f'L{i + 1}'][uid]), dim=-1)\n",
    "            serv_embeds = t.cat((serv_embeds, g.ndata[f'L{i + 1}'][sid+339]), dim=-1)\n",
    "\n",
    "        return user_embeds,serv_embeds\n",
    "\n",
    "\n",
    "class NeuGraphMF(t.nn.Module):\n",
    "\n",
    "    def __init__(self, usergraph, servgraph, dim, order=3):\n",
    "        super(NeuGraphMF, self).__init__()\n",
    "        self.usergraph = usergraph\n",
    "        self.servgraph = servgraph\n",
    "        self.dim = dim\n",
    "        self.order = order\n",
    "        self.UserEmbedding = GraphSAGEConv(usergraph,dim, order)\n",
    "        self.ServEmbedding = GraphSAGEConv(servgraph,dim, order)\n",
    "        self.ncf = NeuralCF(2 * (order + 1) * dim)\n",
    "\n",
    "    def forward(self, uid, sid):\n",
    "        user_embeds = self.UserEmbedding(uid)\n",
    "        serv_embeds = self.ServEmbedding(sid)\n",
    "        #return t.sum(user_embeds * serv_embeds,-1).sigmoid()\n",
    "        return self.ncf(t.cat((user_embeds, serv_embeds), dim=-1)).sigmoid()\n",
    "\n",
    "class NeuGraphMF_SG(t.nn.Module):\n",
    "\n",
    "    def __init__(self, graph, dim, order=3):\n",
    "        super(NeuGraphMF_SG, self).__init__()\n",
    "        self.graph = graph\n",
    "        self.dim = dim\n",
    "        self.order = order\n",
    "        self.GlobalEmbedding = GraphSAGEConvSG(graph,dim, order)\n",
    "        self.ncf = NeuralCF(2 * (order + 1) * dim)\n",
    "\n",
    "    def forward(self, uid, sid):\n",
    "        user_embeds, serv_embeds = self.GlobalEmbedding(uid,sid)\n",
    "        #return t.sum(user_embeds * serv_embeds,-1).sigmoid()\n",
    "        return self.ncf(t.cat((user_embeds, serv_embeds), dim=-1)).sigmoid()\n",
    "cuda = t.cuda.is_available()\n",
    "#model = NeuGraphMF(userg, servg, dim=32, order=2)\n",
    "model = NeuGraphMF_SG(extg, dim=32, order=2)\n",
    "\n",
    "if cuda:\n",
    "    model = NeuGraphMF(userg, servg, dim=32, order=0).cuda()\n",
    "    # model = PureMF(dim=64).cuda()\n",
    "\n",
    "loss = t.nn.L1Loss().cuda()\n",
    "lr = 1e-3\n",
    "optimizer = t.optim.AdamW(model.parameters(),lr=lr)\n",
    "\n",
    "for epoch in range(15):\n",
    "    total_loss = 0\n",
    "    if epoch % 5 == 0:\n",
    "        lr /= 2\n",
    "        optimizer = t.optim.AdamW(model.parameters(),lr=lr,weight_decay=1e-3)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    try:\n",
    "        with tqdm(trainLoader) as dataiter:\n",
    "            for batch in dataiter:\n",
    "                optimizer.zero_grad()\n",
    "                uid,sid,label = batch\n",
    "                if cuda:\n",
    "                    uid = uid.cuda()\n",
    "                    sid = sid.cuda()\n",
    "                    label = label.cuda()\n",
    "                val = model(uid,sid)\n",
    "                val = val.reshape(label.shape)\n",
    "                trainloss = loss(val,label)\n",
    "                trainloss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += trainloss\n",
    "    except KeyboardInterrupt:\n",
    "        dataiter.close()\n",
    "        raise\n",
    "    arr0 = np.zeros((5,))\n",
    "    model.eval()\n",
    "    try:\n",
    "        with tqdm(testLoader) as dataiter:\n",
    "            for batch in dataiter:\n",
    "                uid,sid,label = batch\n",
    "                if cuda:\n",
    "                    uid = uid.cuda()\n",
    "                    sid = sid.cuda()\n",
    "                val = model(uid,sid)\n",
    "                val = val.reshape(label.shape).detach().cpu().numpy()\n",
    "                val,label = val * 19.9,label * 19.9\n",
    "                label = label.detach().numpy()\n",
    "                arr0 += Metrics(val,label)\n",
    "    except KeyboardInterrupt:\n",
    "        dataiter.close()\n",
    "        raise\n",
    "    arr0 /= len(testLoader)\n",
    "    print(f'Epochs {epoch}: Loss={total_loss / len(trainLoader)}')\n",
    "    print('Epochs %d: MAE:%.4f NMAE:%.4f RMSE:%.4f MRE:%.4f NPRE:%.4f' %(epoch,*arr0))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=0) #使用TSNE对特征降到二维\n",
    "result = tsne.fit_transform(userg.ndata['L0'][:339].detach().numpy()) #降维后的数据\n",
    "plt.scatter(result[:,0],result[:,1])\n",
    "plt.show()\n",
    "\n",
    "# Attention Model\n",
    "\n",
    "\n",
    "class SAGEConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 aggregator_type,\n",
    "                 feat_drop=0.,\n",
    "                 bias=True,\n",
    "                 norm=None,\n",
    "                 activation=None):\n",
    "        super(SAGEConv, self).__init__()\n",
    "\n",
    "        #self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n",
    "        self._out_feats = out_feats\n",
    "        self._aggre_type = aggregator_type\n",
    "        self.norm = norm\n",
    "        self.feat_drop = nn.Dropout(feat_drop)\n",
    "        self.activation = activation\n",
    "        # aggregator type: mean/pool/lstm/gcn\n",
    "        if aggregator_type == 'pool':\n",
    "            self.fc_pool = nn.Linear(self._in_src_feats, self._in_src_feats)\n",
    "        if aggregator_type == 'lstm':\n",
    "            self.lstm = nn.LSTM(self._in_src_feats, self._in_src_feats, batch_first=True)\n",
    "        if aggregator_type != 'gcn':\n",
    "            self.fc_self = nn.Linear(self._in_dst_feats, out_feats, bias=bias)\n",
    "        self.fc_neigh = nn.Linear(self._in_src_feats, out_feats, bias=bias)\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(dim,num_heads=1)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        if self._aggre_type == 'pool':\n",
    "            nn.init.xavier_uniform_(self.fc_pool.weight, gain=gain)\n",
    "        if self._aggre_type == 'lstm':\n",
    "            self.lstm.reset_parameters()\n",
    "        if self._aggre_type != 'gcn':\n",
    "            nn.init.xavier_uniform_(self.fc_self.weight, gain=gain)\n",
    "        nn.init.xavier_uniform_(self.fc_neigh.weight, gain=gain)\n",
    "\n",
    "    def _lstm_reducer(self, nodes):\n",
    "        \"\"\"LSTM reducer\n",
    "        NOTE(zihao): lstm reducer with default schedule (degree bucketing)\n",
    "        is slow, we could accelerate this with degree padding in the future.\n",
    "        \"\"\"\n",
    "        m = nodes.mailbox['m'] # (B, L, D)\n",
    "        batch_size = m.shape[0]\n",
    "        h = (m.new_zeros((1, batch_size, self._in_src_feats)),\n",
    "             m.new_zeros((1, batch_size, self._in_src_feats)))\n",
    "        _, (rst, _) = self.lstm(m, h)\n",
    "        return {'neigh': rst.squeeze(0)}\n",
    "\n",
    "    def forward(self, graph, feat):\n",
    "        r\"\"\"Compute GraphSAGE layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        graph : DGLGraph\n",
    "            The graph.\n",
    "        feat : torch.Tensor or pair of torch.Tensor\n",
    "            If a torch.Tensor is given, the input feature of shape :math:`(N, D_{in})` where\n",
    "            :math:`D_{in}` is size of input feature, :math:`N` is the number of nodes.\n",
    "            If a pair of torch.Tensor is given, the pair must contain two tensors of shape\n",
    "            :math:`(N_{in}, D_{in_{src}})` and :math:`(N_{out}, D_{in_{dst}})`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output feature of shape :math:`(N, D_{out})` where :math:`D_{out}`\n",
    "            is size of output feature.\n",
    "        \"\"\"\n",
    "        graph = graph.local_var()\n",
    "\n",
    "        if isinstance(feat, tuple):\n",
    "            feat_src = self.feat_drop(feat[0])\n",
    "            feat_dst = self.feat_drop(feat[1])\n",
    "        else:\n",
    "            feat_src = feat_dst = self.feat_drop(feat)\n",
    "\n",
    "        h_self = feat_dst\n",
    "\n",
    "        if self._aggre_type == 'mean':\n",
    "            graph.srcdata['h'] = feat_src\n",
    "            graph.update_all(fn.copy_src('h', 'm'), fn.mean('m', 'neigh'))\n",
    "            h_neigh = graph.dstdata['neigh']\n",
    "        elif self._aggre_type == 'gcn':\n",
    "            #check_eq_shape(feat)\n",
    "            graph.srcdata['h'] = feat_src\n",
    "            graph.dstdata['h'] = feat_dst     # same as above if homogeneous\n",
    "            graph.update_all(fn.copy_src('h', 'm'), fn.sum('m', 'neigh'))\n",
    "            # divide in_degrees\n",
    "            degs = graph.in_degrees().to(feat_dst)\n",
    "            h_neigh = (graph.dstdata['neigh'] + graph.dstdata['h']) / (degs.unsqueeze(-1) + 1)\n",
    "        elif self._aggre_type == 'pool':\n",
    "            graph.srcdata['h'] = F.relu(self.fc_pool(feat_src))\n",
    "            graph.update_all(fn.copy_src('h', 'm'), fn.max('m', 'neigh'))\n",
    "            h_neigh = graph.dstdata['neigh']\n",
    "        elif self._aggre_type == 'lstm':\n",
    "            graph.srcdata['h'] = feat_src\n",
    "            graph.update_all(fn.copy_src('h', 'm'), self._lstm_reducer)\n",
    "            h_neigh = graph.dstdata['neigh']\n",
    "        else:\n",
    "            raise KeyError('Aggregator type {} not recognized.'.format(self._aggre_type))\n",
    "\n",
    "        # GraphSAGE GCN does not require fc_self.\n",
    "        if self._aggre_type == 'gcn':\n",
    "            rst = self.fc_neigh(h_neigh)\n",
    "        else:\n",
    "            rst = self.fc_self(h_self) + self.fc_neigh(h_neigh)\n",
    "        # activation\n",
    "        if self.activation is not None:\n",
    "            rst = self.activation(rst)\n",
    "        # normalization\n",
    "        if self.norm is not None:\n",
    "            rst = self.norm(rst)\n",
    "        return rst\n",
    "extg.in_degrees()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
